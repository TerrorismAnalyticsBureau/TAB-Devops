{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = \"attacktype1\"\n",
    "\n",
    "NUMERIC_COLS = [\n",
    "    \"iyear\",\n",
    "    \"imonth\",\n",
    "    \"iday\",\n",
    "    \"country\",\n",
    "    \"region_code\",\n",
    "]\n",
    "\n",
    "CAT_NOM_COLS = [\n",
    "    \"provstate\",\n",
    "    \"city\",\n",
    "]\n",
    "\n",
    "CAT_ORD_COLS = [\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ Data Preprocessing ------------------------------\n",
    "\n",
    "# Set pyTorch local env to use segmented GPU memory\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Clear GPU cache & Set the device to use GPU\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "# Skip rows = 1 because those are the column names\n",
    "X = np.array([])\n",
    "\n",
    "# Read the file using its encoding\n",
    "data = pd.read_csv('./globalterrorismdb_0718dist.csv', encoding=\"Windows-1252\")\n",
    "\n",
    "# Extract relevant columns (adjust indices or column names as needed)\n",
    "input_columns = data.iloc[:, [1, 2, 3, 7, 11]]\n",
    "input_columns = input_columns.fillna(0)\n",
    "\n",
    "# Convert non-numeric to numeric and fill missing values\n",
    "for col in input_columns.columns:\n",
    "    input_columns[col] = pd.to_numeric(input_columns[col], errors='coerce')  # Convert non-numeric to NaN\n",
    "input_columns = input_columns.fillna(0)  # Replace NaN with 0\n",
    "\n",
    "attack_target = data.iloc[:, [28]]\n",
    "group_target = data.iloc[:, [58]]\n",
    "\n",
    "# Set the base date (last day of 2017)\n",
    "last_date = datetime(2017, 12, 31)\n",
    "\n",
    "# Convert last date to numeric form\n",
    "last_date_numeric = last_date.toordinal()\n",
    "\n",
    "# Get date from dataset\n",
    "data['date_str'] = data['iyear'].astype(str) + '-' + data['imonth'].astype(str).str.zfill(2) + '-' + data['iday'].astype(str).str.zfill(2)\n",
    "data['date'] = pd.to_datetime(data['date_str'], errors='coerce')\n",
    "\n",
    "\n",
    "# Convert dates to numeric by subtracting the last date of 2017\n",
    "# Get number of days since Dec 31, 2017\n",
    "data['date_numeric'] = (data['date'] - last_date).dt.days\n",
    "\n",
    "# Extract unique values\n",
    "unique_attacks = list(set(data['attacktype1_txt']))\n",
    "unique_groups = list(set(data['gname']))\n",
    "unique_provstates = list(set(data['provstate']))\n",
    "unique_cities = list(set(data['city']))\n",
    "\n",
    "# Initialize LabelEncoder and fit to the unique groups\n",
    "attack_encoder = LabelEncoder()\n",
    "attack_encoder.fit(unique_attacks)\n",
    "\n",
    "group_encoder = LabelEncoder()\n",
    "group_encoder.fit(unique_groups)\n",
    "\n",
    "provstate_encoder = LabelEncoder()\n",
    "provstate_encoder.fit(unique_provstates)\n",
    "\n",
    "city_encoder = LabelEncoder()\n",
    "city_encoder.fit(unique_cities)\n",
    "\n",
    "# Set the output size based on the number of unique attack types\n",
    "num_attack_types = len(unique_attacks)\n",
    "num_groups = len(unique_groups)\n",
    "num_cities = len(unique_cities)\n",
    "num_provstates = len(unique_provstates)\n",
    "\n",
    "# Create a dictionary to map names to their encoded IDs\n",
    "group_dict = pd.Series(group_encoder.transform(unique_groups), index=unique_groups)\n",
    "provstate_dict = pd.Series(provstate_encoder.transform(unique_provstates), index=unique_provstates)\n",
    "city_dict = pd.Series(city_encoder.transform(unique_cities), index=unique_cities)\n",
    "\n",
    "# Assign values to tensors\n",
    "input_tensor = torch.tensor(input_columns.to_numpy(), dtype=torch.float32)\n",
    "attack_target_tensor = torch.tensor(attack_target.values, dtype=torch.float32)\n",
    "group_target_tensor = torch.tensor(group_encoder.fit_transform(group_target.values), dtype=torch.float32)\n",
    "city_target_tensor = torch.tensor(city_encoder.fit_transform(data['city'].values), dtype=torch.float32)\n",
    "provstate_target_tensor = torch.tensor(provstate_encoder.fit_transform(data['provstate'].values), dtype=torch.float32)\n",
    "\n",
    "# TESTING - PRINT DICTIONARY ITEMS\n",
    "#for key, value in group_dict.items():\n",
    "#  print(\"group: \", key, \"| ID #:\", value)\n",
    "\n",
    "#for key, value in provstate_dict.items():\n",
    "#  print(\"provstate: \", key, \"| ID #:\", value)\n",
    "\n",
    "#for key, value in city_dict.items():\n",
    "#  print(\"city: \", key, \"| ID #:\", value)\n",
    "\n",
    "# Assign values to tensors for processing\n",
    "X_tensor = input_tensor\n",
    "\n",
    "# Normalize: mean and std for each feature\n",
    "mean = X_tensor.mean(dim=0, keepdim=True)\n",
    "std = X_tensor.std(dim=0, keepdim=True)\n",
    "X_tensor = (X_tensor - mean) / std\n",
    "\n",
    "Y_tensor_attack = attack_target_tensor\n",
    "Y_tensor_group = group_target_tensor\n",
    "Y_tensor_city = city_target_tensor\n",
    "Y_tensor_provstate = provstate_target_tensor\n",
    "Y_tensor_date = torch.tensor(data['date_numeric'] - last_date_numeric, dtype=torch.float32)\n",
    "\n",
    "# Set tensors to use GPU\n",
    "X_tensor = X_tensor.to(device)\n",
    "Y_tensor_attack = Y_tensor_attack.to(device)\n",
    "Y_tensor_group = Y_tensor_group.to(device)\n",
    "Y_tensor_city = Y_tensor_city.to(device)\n",
    "Y_tensor_provstate = Y_tensor_provstate.to(device)\n",
    "Y_tensor_date = Y_tensor_date.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Define Arguments for this step\n",
    "\n",
    "class MyArgs:\n",
    "    def __init__(self, /, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = MyArgs(\n",
    "            raw_data = \"../../data/\", \n",
    "            train_data = \"/tmp/prep/train\",\n",
    "            val_data = \"/tmp/prep/val\",\n",
    "            test_data = \"/tmp/prep/test\",\n",
    "            )\n",
    "\n",
    "os.makedirs(args.train_data, exist_ok = True)\n",
    "os.makedirs(args.val_data, exist_ok = True)\n",
    "os.makedirs(args.test_data, exist_ok = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(args):\n",
    "    '''Read, split, and save datasets'''\n",
    "\n",
    "    # ------------ Reading Data ------------ #\n",
    "    # -------------------------------------- #\n",
    "\n",
    "    print(\"mounted_path files: \")\n",
    "    arr = os.listdir(args.raw_data)\n",
    "    print(arr)\n",
    "\n",
    "    data = pd.read_csv((Path(args.raw_data) / 'globalterrorismdb_0718dist.csv'))\n",
    "    data = data[NUMERIC_COLS + CAT_NOM_COLS + CAT_ORD_COLS + [TARGET_COL]]\n",
    "\n",
    "    # ------------- Split Data ------------- #\n",
    "    # -------------------------------------- #\n",
    "\n",
    "    # Split data into train, val and test datasets\n",
    "\n",
    "    random_data = np.random.rand(len(data))\n",
    "\n",
    "    msk_train = random_data < 0.7\n",
    "    msk_val = (random_data >= 0.7) & (random_data < 0.85)\n",
    "    msk_test = random_data >= 0.85\n",
    "\n",
    "    train = data[msk_train]\n",
    "    val = data[msk_val]\n",
    "    test = data[msk_test]\n",
    "\n",
    "    mlflow.log_metric('train size', train.shape[0])\n",
    "    mlflow.log_metric('val size', val.shape[0])\n",
    "    mlflow.log_metric('test size', test.shape[0])\n",
    "\n",
    "    train.to_parquet((Path(args.train_data) / \"train.parquet\"))\n",
    "    val.to_parquet((Path(args.val_data) / \"val.parquet\"))\n",
    "    test.to_parquet((Path(args.test_data) / \"test.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data path: ../../data/\n",
      "Train dataset output path: /tmp/prep/train\n",
      "Val dataset output path: /tmp/prep/val\n",
      "Test dataset path: /tmp/prep/test\n",
      "mounted_path files: \n",
      "['taxi-batch.csv', 'taxi-data.csv', 'taxi-request.json']\n"
     ]
    }
   ],
   "source": [
    "mlflow.start_run()\n",
    "\n",
    "lines = [\n",
    "    f\"Raw data path: {args.raw_data}\",\n",
    "    f\"Train dataset output path: {args.train_data}\",\n",
    "    f\"Val dataset output path: {args.val_data}\",\n",
    "    f\"Test dataset path: {args.test_data}\",\n",
    "\n",
    "]\n",
    "\n",
    "for line in lines:\n",
    "    print(line)\n",
    "\n",
    "main(args)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Local Disk\n",
      " Volume Serial Number is 583C-74B4\n",
      "\n",
      " Directory of c:\\tmp\\prep\\train\n",
      "\n",
      "10/07/2022  12:08 AM    <DIR>          .\n",
      "10/07/2022  12:08 AM    <DIR>          ..\n",
      "10/07/2022  12:08 AM           277,190 train.parquet\n",
      "               1 File(s)        277,190 bytes\n",
      "               2 Dir(s)  788,218,421,248 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls \"/tmp/prep/train\" "
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "local-env"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c87d6401964827bd736fe8e727109b953dd698457ca58fb5acabab22fd6dac41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
